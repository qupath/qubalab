{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4f046e-a6e0-491f-8ff8-df88208c60de",
   "metadata": {},
   "source": [
    "# Working with objects\n",
    "\n",
    "This notebook will show how to exchange objects (e.g. annotations, detections) between QuPath and Python.\n",
    "\n",
    "As we will communicate with QuPath, it is recommended to go through the *communicating_with_qupath.ipynb* notebook first. Also, as we will work with images, it is recommended to go through the *opening_images.ipynb* notebook first.\n",
    "\n",
    "A few classes and functions of the QuBaLab package will be presented in this notebook. For more details on them, you can go to the documentation on https://qupath.github.io/qubalab/ and type a class/function name in the search bar. You will then see details on the parameters that functions take.\n",
    "\n",
    "Before running this notebook, you should launch QuPath, start the Py4J gateway, and open an image that has at least one annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57058266-2e94-49eb-9d7d-34e06ab0ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubalab.qupath import qupath_gateway\n",
    "\n",
    "token = None   # change the value of this variable if you provided a token while creating the QuPath gateway\n",
    "port = 25333   # change the value of this variable if you provided a different port while creating the QuPath gateway\n",
    "gateway = qupath_gateway.create_gateway(auth_token=token, port=port)\n",
    "\n",
    "print(\"Gateway created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc856f-9bec-4a1a-ba7c-bb4dd07ab0f1",
   "metadata": {},
   "source": [
    "## Get objects from QuPath\n",
    "\n",
    "We will first see how to get objects from QuPath. As mentioned in the *communicating_with_qupath.ipynb* notebook, there are two ways to communicate with QuPath:\n",
    "- Use one of the functions of `qubalab.qupath.qupath_gateway`.\n",
    "- If no function exists for your use case, use `gateway.entry_point`.\n",
    "\n",
    "In our case, the `qubalab.qupath.qupath_gateway` file has a `get_objects()` function that suits our goal, so we will use it.\n",
    "\n",
    "This function has an `object_type` parameter to define which type of object to retrieve. We will work with annotations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b685d-3df9-49d6-a926-210b84994bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubalab.objects.object_type import ObjectType\n",
    "\n",
    "object_type = ObjectType.ANNOTATION    # could be DETECTION, TILE, CELL, TMA_CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a6939-0fcb-4ee2-bdf5-f805aa7ad71d",
   "metadata": {},
   "source": [
    "### Get annotations as `JavaObject`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee160d-76e8-4b10-8e5a-abe5e67ec433",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = qupath_gateway.get_objects(object_type = object_type)\n",
    "\n",
    "for annotation in annotations:\n",
    "    print(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9adea-c292-4967-a7d0-304258435e78",
   "metadata": {},
   "source": [
    "By default, the returned objects are Java objects.\n",
    "\n",
    "This is useful if we want to do something that isn't supported by the `qubalab.qupath.qupath_gateway` module, but it isn't very convenient... we end up needing to write Python code that looks a *lot* like Java code. We can also get stuck when things get complicated (e.g. due to threading issues) because we don't have the ability to do *everything* Java can do.\n",
    "\n",
    "We *can* make changes though, like setting names and classifications, which is nice.\n",
    "\n",
    "If we do, we should remember to call `qupath_gateway.refresh_qupath()` to update the interface accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a3b04-ab34-4bc4-9610-2e10d565d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = annotations[0]\n",
    "\n",
    "# Change the QuPath annotation\n",
    "annotation.setName(\"Hello from Python\")\n",
    "\n",
    "# Refresh the QuPath interface. You should see the changes in QuPath\n",
    "qupath_gateway.refresh_qupath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbe83a-8d96-47fe-afcf-6b4f286a0eaf",
   "metadata": {},
   "source": [
    "### Get annotations as `GeoJSON`\n",
    "\n",
    "There's another approach we can take. Rather than directly accessing the QuPath objects, we can request them as GeoJSON. This does *not* give direct access, but rather imports a more Python-friendly representation that is no longer connected to QuPath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057b24e-035f-408e-b1ad-7b0228d8643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = qupath_gateway.get_objects(object_type = object_type, converter='geojson')\n",
    "\n",
    "print(type(annotations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03144ae-5060-454e-a910-4864c852b031",
   "metadata": {},
   "source": [
    "In practice, it's really a slightly 'enhanced' GeoJSON representation, called `ImageFeature`, because it includes a few extra fields that are relevant for QuPath.\n",
    "\n",
    "This includes any classification, name, color and object type. It also includes a plane, which stores `z` and `t` indices.\n",
    "But because it is still basically GeoJSON, we can use it with other Python libraries that supports GeoJSON... such as `geojson`.\n",
    "\n",
    "We can also use it with `Shapely`, which is particularly useful. Shapely gives us access to lots of useful methods - and shapely objects can be displayed nicely in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647cfa9d-e816-4204-9db5-c1cdf1bc2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "\n",
    "shape(annotations[0].geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a349d3-127b-4e69-87d8-e47c762b74e0",
   "metadata": {},
   "source": [
    "## Add and remove objects from QuPath\n",
    "\n",
    "The GeoJSON representation doesn't give us direct access to the QuPath objects, but we can still make changes and send them back.\n",
    "The easiest way to see this in action is to begin by deleting the annotations and then adding them back again - but this time with a different color.\n",
    "\n",
    "> We only assign colors to annotations that aren't classified, so that we don't override the colors that QuPath uses for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c166ae-63a7-4fc2-afeb-c32ab65226ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for annotation in annotations:\n",
    "    if not annotation.classification:\n",
    "        annotation.color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "qupath_gateway.delete_objects(object_type = object_type)\n",
    "qupath_gateway.add_objects(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a0faa-29b4-4f8b-9df3-c038d060cabc",
   "metadata": {},
   "source": [
    "## Create masks & labeled images\n",
    "\n",
    "One reason to use Python rather than QuPath/Java is that NumPy/SciPy/scikit-image and other tools make working with pixels easier and more fun.\n",
    "\n",
    "To begin, let's use the GeoJSON representation to create masks and labeled images.\n",
    "\n",
    "To create masks and labeled images, Qubalab has a `LabeledImageServer` class. This class is an implementation of the Qubalab `ImageServer` which is described in the *opening_images.ipynb* notebook, so it is recommended that you go through this notebook first. In short, `ImageServer` is a class to access metadata and pixel values of images.\n",
    "\n",
    "This server needs:\n",
    "- Some metadata representing the image containing the objects. Since we are working with the image that is opened in QuPath, we can read the metadata of the `QuPathServer`, as described in *communicating_with_qupath.ipynb*.\n",
    "- The objects to represent. We will give the annotations we've been working with.\n",
    "- A downsample to apply to the image.\n",
    "\n",
    "Once the server is created, all functions described in *opening_images.ipynb* (such as `read_region()` to read the image) are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caec8f-f900-4e37-bf0e-06c8bb28ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubalab.images.qupath_server import QuPathServer\n",
    "from qubalab.images.labeled_server import LabeledImageServer\n",
    "\n",
    "# Create a QuPathServer that represents the image currently opened in QuPath\n",
    "qupath_server = QuPathServer(gateway)\n",
    "\n",
    "# Set a downsample. The labeled image will be 20 times smaller than the image currently opened in QuPath\n",
    "downsample = 20\n",
    "\n",
    "# Create the LabeledImageServer. This doesn't create labeled image yet\n",
    "labeled_server = LabeledImageServer(qupath_server.metadata, annotations, downsample=downsample)\n",
    "\n",
    "# Request the pixel values of the entire labeled image. Pixel values will be created as they are requested \n",
    "label_image = labeled_server.read_region()\n",
    "\n",
    "\n",
    "# label_image is an image of shape (c, y, x), not easily plottable\n",
    "# We use a utility function from qubalab to plot the image\n",
    "from qubalab.display.plot import plotImage\n",
    "import matplotlib.pyplot as plt\n",
    "_, ax = plt.subplots()\n",
    "plotImage(ax, label_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57ccb3-a06f-4c43-92e9-fb864721008c",
   "metadata": {},
   "source": [
    "By default, the `LabeledImageServer` will return a single channel image where all objects are represented by integer values (labels). It's a labeled image.\n",
    "\n",
    "Another option is to create a multi channel image where each channel is a mask indicating if an annotation is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585d2c5-95ca-49ce-91fe-e87d5777dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another LabeledImageServer, which will represent a stack of masks here\n",
    "mask_server = LabeledImageServer(qupath_server.metadata, annotations, downsample=downsample, multichannel=True)\n",
    "\n",
    "# Compute and return the masks\n",
    "masks = mask_server.read_region()\n",
    "\n",
    "# masks contains (n+1) channels, where n is the number of annotations\n",
    "# The i channel corresponds to the mask representing the i annotation\n",
    "# Let's plot the first mask corresponding to the first annotation\n",
    "_, ax = plt.subplots()\n",
    "plotImage(ax, masks, channel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e20af7-06d5-43b5-a2c9-170afc38da5c",
   "metadata": {},
   "source": [
    "## Image processing & creating objects\n",
    "\n",
    "This whole thing becomes more useful when we start to use Python for image processing.\n",
    "\n",
    "Here we'll use scikit-image to help find objects using two different thresholding methods. We'll then convert them to QuPath objects and add them to the current QuPath viewer for visualization.\n",
    "\n",
    "We will:\n",
    "- Get pixels of the image opened in QuPath. We can use the `qupath_server` variable created before and use the `read_region()` function with a downsample.\n",
    "- Convert the previous image to greyscale, and apply a gaussian filter to it.\n",
    "- For each threshold method:\n",
    "    - Apply the threshold, and create a mask from it.\n",
    "    - Create annotations from the created mask. This uses the qubalab `ImageFeature` class, which represents a GeoJSON object. We actually used this class before: the `qupath_gateway.get_objects(converter='geojson')` function returned a list of `ImageFeature`.\n",
    "    - Add the annotations to QuPath.\n",
    "    - Plot the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9f415-8100-4e79-b459-1c1cebc0afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import gaussian, threshold_otsu, threshold_triangle\n",
    "from skimage.color import rgb2gray\n",
    "from qubalab.objects.image_feature import ImageFeature\n",
    "\n",
    "\n",
    "# Set a downsample. The labeled image will be 20 times smaller than the image currently opened in QuPath\n",
    "downsample = 20\n",
    "\n",
    "# Set different threshold methods to apply to the image\n",
    "threshold_methods = {\n",
    "    'Otsu' : threshold_otsu,\n",
    "    'Triangle' : threshold_triangle\n",
    "}\n",
    "\n",
    "# Read image opened in QuPath\n",
    "image = qupath_server.read_region(downsample=downsample)\n",
    "\n",
    "# Convert the image to greyscale\n",
    "if qupath_server.metadata.is_rgb:\n",
    "    # If the image is RGB, we convert it to grayscale\n",
    "    # read_region() returns an image with the (c, y, x) shape.\n",
    "    # To use rgb2gray, we need to move the channel axis so that\n",
    "    # the shape becomes (y, x, c)\n",
    "    image = np.moveaxis(image, 0, -1)\n",
    "    image = rgb2gray(image)\n",
    "else:\n",
    "    # Else, we only consider the first channel of the image\n",
    "    image = image[0, ...]\n",
    "\n",
    "# Apply a gaussian filter\n",
    "image = gaussian(image, 2.0)\n",
    "\n",
    "# Iterate over threshold methods\n",
    "for i, (name, method) in enumerate(threshold_methods.items()):\n",
    "    # Apply threshold to image\n",
    "    threshold = method(image)\n",
    "\n",
    "    # Create mask from threshold\n",
    "    mask = image < threshold\n",
    "\n",
    "    # Create annotations from mask\n",
    "    annotations = ImageFeature.create_from_label_image(\n",
    "        mask,   \n",
    "        scale=downsample,   # mask is 20 times smaller than the QuPath image, so we scale\n",
    "                            # the annotations to fit the QuPath image\n",
    "        classification_names=name,  # set a single classification to the detected annotations\n",
    "    )\n",
    "\n",
    "    # Add annotations to QuPath\n",
    "    qupath_gateway.add_objects(annotations)\n",
    "\n",
    "    # Plot mask\n",
    "    plt.subplot(1, len(threshold_methods), i+1)\n",
    "    plt.imshow(mask)\n",
    "    plt.title(f'{name} (threshold={threshold:.2f})')\n",
    "    plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081e734-d99c-4b53-9ca6-523ea8ee84e4",
   "metadata": {},
   "source": [
    "Here, we used `ImageServer.readRegion()` to get pixels as a numpy array. It is also possible to use `ImageServer.level_to_dask()` or `ImageServer.to_dask` (as explained in the *opening_images.ipynb* notebook), which return **Dask** arrays. Using [Dask](https://docs.dask.org), we can get access to the entire image as a single, NumPy-like array, at any pyramid level - even if it's bigger than our RAM could handle.\n",
    "\n",
    "Let's delete these annotations as we won't use them anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944a5f7-fd8b-4643-bd2a-a92ef3dd3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "qupath_gateway.delete_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae26c27-bd50-4682-813f-db127b472b15",
   "metadata": {},
   "source": [
    "## Displaying objects\n",
    "\n",
    "We don't *need* QuPath to visualize GeoJSON features.\n",
    "\n",
    "QuBaLab also includes functionality for generating matplotlib plots that *look* a lot like QuPath plots... but that don't use QuPath.\n",
    "\n",
    "**Before running the next notebook cell**, you should draw a few small annotations in QuPath and detect cells within them.\n",
    "\n",
    "The plotting code will show the annotations and cells - randomly recoloring the cells, to demonstrate that they are distinct from QuPath's rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779f4d7-6d5d-4560-b86a-74824ebb6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubalab.display.plot import plotImageFeatures\n",
    "\n",
    "# Get QuPath annotations and detections\n",
    "annotations = qupath_gateway.get_objects(object_type = ObjectType.ANNOTATION, converter='geojson')\n",
    "detections = qupath_gateway.get_objects(object_type = ObjectType.DETECTION, converter='geojson')\n",
    "\n",
    "if len(detections) == 0:\n",
    "    print(\"No detections found. Please run cell detection from QuPath before running this cell.\")\n",
    "\n",
    "# Set a random color for each detection\n",
    "for detection in detections:\n",
    "    detection.color = [random.randint(0, 255) for _ in range(3)]\n",
    "\n",
    "# Plot every annotations and their detections\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for i, annotation in enumerate(annotations):\n",
    "    ax = fig.add_subplot(len(annotations), 1, i+1)\n",
    "\n",
    "    # Invert y axis. This is needed because in QuPath, the y-axis is going down\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Set title of graph from annotation name. You won't see it if the annotation doesn't have a name\n",
    "    ax.set_title(annotation.name)\n",
    "\n",
    "    # Plot annotation\n",
    "    plotImageFeatures(ax, [annotation], region=annotation.geometry, fill_opacity=0.1)\n",
    "\n",
    "    # Plot detections that are located below the annotation\n",
    "    plotImageFeatures(ax, detections, region=annotation.geometry, fill_opacity=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd303e6-a5e1-4ae2-876c-09b1c5bcf163",
   "metadata": {},
   "source": [
    "If you combine that with `ImageServer.readRegion()`, you can plot both the image and the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73adc0fe-8d4a-4d65-9b91-cb0832014ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "\n",
    "# An offset in pixels to also see the image around the annotations\n",
    "offset = 100\n",
    "\n",
    "# Plot every annotations, their detections, and the image below it\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for i, annotation in enumerate(annotations):\n",
    "    ax = fig.add_subplot(len(annotations), 1, i+1)\n",
    "\n",
    "    # Set title of graph from annotation name. You won't see it if the annotation doesn't have a name\n",
    "    ax.set_title(annotation.name)\n",
    "\n",
    "    # Compute bounds of the image. We add a small offset to see the image around the annotation\n",
    "    bounds = shape(annotation.geometry).bounds\n",
    "    min_x = max(0, bounds[0] - offset)\n",
    "    min_y = max(0, bounds[1] - offset)\n",
    "    max_x = min(qupath_server.metadata.width, bounds[2] + offset)\n",
    "    max_y = min(qupath_server.metadata.height, bounds[3] + offset)\n",
    "\n",
    "    # Get pixel values of image\n",
    "    image = qupath_server.read_region(x=min_x, y=min_y, width=max_x-min_x, height=max_y-min_y)\n",
    "\n",
    "    # Plot annotation\n",
    "    plotImageFeatures(ax, [annotation], region=annotation.geometry, fill_opacity=0.1)\n",
    "\n",
    "    # Plot detections that are located below the annotation\n",
    "    plotImageFeatures(ax, detections, region=annotation.geometry, fill_opacity=0.25)\n",
    "\n",
    "    # Plot image\n",
    "    plotImage(ax, image, offset=[min_x, min_y, max_x, max_y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
